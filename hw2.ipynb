{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework II - Report\n",
    "\n",
    "Cec√≠lia Correia, 106827\n",
    "<br>Patr√≠cia Gameiro, 107245"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming and critical analysis [7v]\n",
    "\n",
    "**Consider the heart-disease.csv dataset available at the course webpage‚Äôs homework tab.\n",
    "Using `sklearn`, apply a 5-fold stratified cross-validation with shuffling (`random_state=0`) for the\n",
    "assessment of predictive models along this section.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) \n",
    "**Compare the performance of a ùëòùëÅùëÅ with ùëò = 5 and a na√Øve Bayes with Gaussian assumption (consider all remaining parameters as default):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. [1.0v] **Plot two boxplots with the fold accuracies for each classifier. Is there one more stable than the other regarding performance? Why do you think that is the case? Explain.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/heart-disease.csv')\n",
    "\n",
    "X = df.drop(\"target\", axis = 1)\n",
    "y = df[\"target\"]\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier = KNeighborsClassifier(n_neighbors = 5)\n",
    "naive_bayes_classifier = GaussianNB()\n",
    "\n",
    "knn_scores = cross_val_score(knn_classifier, X, y.tolist(), cv = skf, scoring='accuracy')\n",
    "naive_bayes_scores = cross_val_score(naive_bayes_classifier, X, y.tolist(), cv = skf, scoring = 'accuracy')\n",
    "\n",
    "plt.figure(figsize = (15, 12))\n",
    "\n",
    "sns.boxplot(data = [knn_scores, naive_bayes_scores], palette = \"Set2\")\n",
    "plt.xticks([0, 1], [\"kNN (k=5)\", \"Na√Øve Bayes\"], fontsize = 11)\n",
    "plt.ylabel('Accuracy', fontsize = 12)\n",
    "\n",
    "# title: Accuracy Comparison between kNN (k=5) and Naive Bayes\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation here!!! \\\n",
    ". \\\n",
    ". \\\n",
    ". \\\n",
    ". \\\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. [1.0v] **Report the accuracy of both models, this time scaling the data with a Min-Max scaler before training the models. Explain the impact that this preprocessing step has on the performance of each model, providing an explanation for the results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. [1.0v] **Using `scipy`, test the hypothesis ‚Äúthe ùëòùëÅùëÅ model is statistically superior to Na√Øve Bayes regarding accuracy‚Äù, asserting whether it is true.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.ttest_rel(knn_scores , naive_bayes_scores , alternative=\"greater\")\n",
    "print(\"knn > naive_bayes? pval=\", res.pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will conduct a one-tailed test using the accuracy scores obtained from our previous analysis. We are considering the following hypotheses:\n",
    "\n",
    "$ùêª_{0}$ : $accuracy_{ùëòNN}$ = $accuracy_{Naive Bayes}$ \\\n",
    "$ùêª_{1}$ : $accuracy_{ùëòNN}$ > $accuracy_{Naive Bayes}$ \n",
    "\n",
    "Using scipy, we calculated a p-value of ‚âà 0.9987.\n",
    "This high p-value indicates that we fail to reject the $ùêª_{0}$ at all conventional significance levels (1%, 5%, and 10%). Therefore, we cannot conclude that kNN is statistically superior to Na√Øve Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) \n",
    "**Using a 80-20 train-test split, vary the number of neighbors of a ùëòùëÅùëÅ classifier using ùëò = {1, 5, 10, 20, 30}. Additionally, for each ùëò, train one classifier using uniform weights and distance weights.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. [1.0v] **Plot the train and test accuracy for each model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_VALUES = [1, 5, 10, 20, 30]\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "            X, y, train_size = 0.8, stratify = y, random_state = 0\n",
    "        )\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for k in K_VALUES:\n",
    "    knn_uniform = KNeighborsClassifier(n_neighbors = k, weights = \"uniform\")\n",
    "    knn_uniform.fit(X_train, y_train)\n",
    "\n",
    "    train_pred_uniform = knn_uniform.predict(X_train)\n",
    "    test_pred_uniform = knn_uniform.predict(X_test)\n",
    "    \n",
    "    train_acc_uniform = accuracy_score(y_train, train_pred_uniform)\n",
    "    test_acc_uniform = accuracy_score(y_test, test_pred_uniform)\n",
    "    \n",
    "    train_accuracies.append(train_acc_uniform)\n",
    "    test_accuracies.append(test_acc_uniform)\n",
    "\n",
    "train_accuracies_dist = []\n",
    "test_accuracies_dist = []\n",
    "\n",
    "for k in K_VALUES:\n",
    "\n",
    "    knn_distance = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "    knn_distance.fit(X_train, y_train)\n",
    "\n",
    "    train_pred_distance = knn_distance.predict(X_train)\n",
    "    test_pred_distance = knn_distance.predict(X_test)\n",
    "\n",
    "    train_acc_distance = accuracy_score(y_train, train_pred_distance)\n",
    "    test_acc_distance = accuracy_score(y_test, test_pred_distance)\n",
    "    \n",
    "    train_accuracies_dist.append(train_acc_distance)\n",
    "    test_accuracies_dist.append(test_acc_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 5)) \n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.plot(\n",
    "    K_VALUES,\n",
    "    train_accuracies,  \n",
    "    label = \"Training Accuracy (Uniform)\",\n",
    "    marker = \"+\",\n",
    "    color = \"#4caf50\",\n",
    ")\n",
    "plt.plot(\n",
    "    K_VALUES,\n",
    "    test_accuracies,\n",
    "    label = \"Test Accuracy (Uniform)\",\n",
    "    marker = \".\",\n",
    "    color = \"#ff5722\",\n",
    ")\n",
    "plt.xlabel(\"Number of Neighbors (k)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Train and Test Accuracy for kNN Classifier (Uniform Weights)\")\n",
    "plt.xticks(K_VALUES)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(\n",
    "    K_VALUES, \n",
    "    train_accuracies_dist, \n",
    "    label = \"Train Accuracy (Distance)\", \n",
    "    marker = \"+\",\n",
    "    color = \"#4caf50\",\n",
    ")\n",
    "plt.plot(\n",
    "    K_VALUES,\n",
    "    test_accuracies_dist, \n",
    "    label = \"Test Accuracy (Distance)\", \n",
    "    marker = \".\",\n",
    "    color = \"#ff5722\",\n",
    ")\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Test Accuracy for kNN Classifier (Distance Weights)')\n",
    "plt.xticks(K_VALUES)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. [1.5v] **Explain the impact of increasing the neighbors on the generalization ability of the models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [1.5V]\n",
    "\n",
    "**Considering the unique properties of the `heart-disease.csv` dataset, identify two possible difficulties of the Na√Øve Bayes model used in the previous exercises when learning from the given dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLACEHOLDER: \n",
    "\n",
    "Na√Øve Bayes assumes that all predictors are independent, which it might not be true with this dataset. <br>\n",
    "The classifier may be biased towards the majority class, which could be problematic in medical datasets, such as this one. <br>\n",
    "If, in the test data, there's a class that is not in the training data, it is possible to end up with zero class probabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
